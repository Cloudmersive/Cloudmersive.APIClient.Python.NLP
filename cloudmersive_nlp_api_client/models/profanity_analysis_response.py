# coding: utf-8

"""
    nlpapiv2

    The powerful Natural Language Processing APIs (v2) let you perform part of speech tagging, entity identification, sentence parsing, and much more to help you understand the meaning of unstructured text.  # noqa: E501

    OpenAPI spec version: v1
    
    Generated by: https://github.com/swagger-api/swagger-codegen.git
"""


import pprint
import re  # noqa: F401

import six


class ProfanityAnalysisResponse(object):
    """NOTE: This class is auto generated by the swagger code generator program.

    Do not edit the class manually.
    """

    """
    Attributes:
      swagger_types (dict): The key is attribute name
                            and the value is attribute type.
      attribute_map (dict): The key is attribute name
                            and the value is json key in definition.
    """
    swagger_types = {
        'successful': 'bool',
        'profanity_score_result': 'float',
        'sentence_count': 'int'
    }

    attribute_map = {
        'successful': 'Successful',
        'profanity_score_result': 'ProfanityScoreResult',
        'sentence_count': 'SentenceCount'
    }

    def __init__(self, successful=None, profanity_score_result=None, sentence_count=None):  # noqa: E501
        """ProfanityAnalysisResponse - a model defined in Swagger"""  # noqa: E501

        self._successful = None
        self._profanity_score_result = None
        self._sentence_count = None
        self.discriminator = None

        if successful is not None:
            self.successful = successful
        if profanity_score_result is not None:
            self.profanity_score_result = profanity_score_result
        if sentence_count is not None:
            self.sentence_count = sentence_count

    @property
    def successful(self):
        """Gets the successful of this ProfanityAnalysisResponse.  # noqa: E501

        True if the profanity detection operation was successful, false otherwise  # noqa: E501

        :return: The successful of this ProfanityAnalysisResponse.  # noqa: E501
        :rtype: bool
        """
        return self._successful

    @successful.setter
    def successful(self, successful):
        """Sets the successful of this ProfanityAnalysisResponse.

        True if the profanity detection operation was successful, false otherwise  # noqa: E501

        :param successful: The successful of this ProfanityAnalysisResponse.  # noqa: E501
        :type: bool
        """

        self._successful = successful

    @property
    def profanity_score_result(self):
        """Gets the profanity_score_result of this ProfanityAnalysisResponse.  # noqa: E501

        Profanity classification score between 0.0 and 1.0 where scores closer to zero have a low probability of being profane or contain obscene language, while scores close to 1.0 have a high probability of being profane or containing obscene language.  Values above 0.8 have a very high probability of being profane.  # noqa: E501

        :return: The profanity_score_result of this ProfanityAnalysisResponse.  # noqa: E501
        :rtype: float
        """
        return self._profanity_score_result

    @profanity_score_result.setter
    def profanity_score_result(self, profanity_score_result):
        """Sets the profanity_score_result of this ProfanityAnalysisResponse.

        Profanity classification score between 0.0 and 1.0 where scores closer to zero have a low probability of being profane or contain obscene language, while scores close to 1.0 have a high probability of being profane or containing obscene language.  Values above 0.8 have a very high probability of being profane.  # noqa: E501

        :param profanity_score_result: The profanity_score_result of this ProfanityAnalysisResponse.  # noqa: E501
        :type: float
        """

        self._profanity_score_result = profanity_score_result

    @property
    def sentence_count(self):
        """Gets the sentence_count of this ProfanityAnalysisResponse.  # noqa: E501

        Number of sentences in input text  # noqa: E501

        :return: The sentence_count of this ProfanityAnalysisResponse.  # noqa: E501
        :rtype: int
        """
        return self._sentence_count

    @sentence_count.setter
    def sentence_count(self, sentence_count):
        """Sets the sentence_count of this ProfanityAnalysisResponse.

        Number of sentences in input text  # noqa: E501

        :param sentence_count: The sentence_count of this ProfanityAnalysisResponse.  # noqa: E501
        :type: int
        """

        self._sentence_count = sentence_count

    def to_dict(self):
        """Returns the model properties as a dict"""
        result = {}

        for attr, _ in six.iteritems(self.swagger_types):
            value = getattr(self, attr)
            if isinstance(value, list):
                result[attr] = list(map(
                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
                    value
                ))
            elif hasattr(value, "to_dict"):
                result[attr] = value.to_dict()
            elif isinstance(value, dict):
                result[attr] = dict(map(
                    lambda item: (item[0], item[1].to_dict())
                    if hasattr(item[1], "to_dict") else item,
                    value.items()
                ))
            else:
                result[attr] = value
        if issubclass(ProfanityAnalysisResponse, dict):
            for key, value in self.items():
                result[key] = value

        return result

    def to_str(self):
        """Returns the string representation of the model"""
        return pprint.pformat(self.to_dict())

    def __repr__(self):
        """For `print` and `pprint`"""
        return self.to_str()

    def __eq__(self, other):
        """Returns true if both objects are equal"""
        if not isinstance(other, ProfanityAnalysisResponse):
            return False

        return self.__dict__ == other.__dict__

    def __ne__(self, other):
        """Returns true if both objects are not equal"""
        return not self == other
